Submission Title,Submission Url,Plain Description,Video,Website,File Url,Desired Prizes,Built With,Github,Mlh Points,Mlh Hardware Lab,Mlh Software Lab,Submitter Screen Name,College/Universities Of Team Members,Additional Team Member Count,Team Member 1 Screen Name,...,
Dibs,https://hophacks-fall-2019.devpost.com/submissions/126465-dibs,"Inspiration

As a student residing in the Baltimore Area, I think two of the largest issues are violent crime and poverty. Furthermore, organizations are frequently seeking helping hands to volunteer for their events. These issues inspired us to build this app which attempts to help connect the gap between those in poverty, charitible organizations, and potential volunteers.

What it does

This app allows charities to post information about when and where they are going to be holding their events. For example, a soup kitchen could make an “event” with a certain address, date, and description. Then people in need of food, now aware of the “event,” are then able to go to the location and make use of much needed resources.

Additionally, under the “details” of this event, people in a position to donate resources can do so. They can specify an item description as well as quantity, giving the charitable organization a good idea of how many extra resources may be available. Furthermore, people can sign up as volunteers allowing organizations to preview how many helping hands they may receive.

How I built it

The core functionality of this app is built with JavaScipt and Firestore’s NoSQL database, and the UI is built with HTML/CSS and some aid from the bootstrap framework. Our database structure is composed of a “event-list” collection. The event collection has documents which represents the events, and each of these documents’ fields holds relevant information: address, date, description, volunteers, etc.

We also attempted to use the google maps API to plot event address, but we were not able to implement this successfully.

Challenges I ran into

I ran into quite a few challenges. Most of these revolved around the way Firebase requires developers to interact with it. For example, for me to sync the adding and deleting of events in real time, I had to follow the specific mechanism that Firebase uses (Snapshots, Changes, Event Listeners, etc.). However, after carefully reading documentation, some patience, and plenty of trial and error, I was able to solve all these issues.

Our biggest challenge was interacting with the google maps API. We attempted to integrate the plotting of addresses to a map, but this caused many issues. For example, we had to use a different mapquest api to convert street addresses to the latitudes and longitudes google maps requires in order to plot points. This is remained unresolved and would be the first thing we would tackle if we had more time.

Accomplishments that I'm proud of

I am most proud of how I was able to integrate an end to end web app with a simple, but functional design. Users have access to useful information that is concisely displayed. I think that anyone who looked at this app would be able to navigate and use it. I am also proud of how I was able to problem solve the vast amount of issues that occurred when I was building this app.

What I learned

The biggest thing I learned was interacting was Firebase. I had done a mini project with it before allowing me to quickly set up and connect to Firestore; however, I had never made as extensive use of the functionality as I did in this project. I had a great time learning about the collection-document structure of FireStore, and I think this would be a great springboard for me to learn about more feature rich but less friendly NoSQL databases like Mongo.

I also learned how to manipulate DOM elements with Javascript. In previous web apps I would hardcode a significant amount of the webpage content in HTML. This time I challenged myself to use JavaScript wherever possible to manipulate components on the webpage. I learned that this is a far more scalable technique, and something I should definitely explore more perhaps with a JavaScript library like React.

What's next for Dibs

I see two clear next steps for Dibs. The first would be to finish the functionality of the Google Maps API. Although people could just plug the address into Google Maps themselves, I think it follows the theme of our user interface that all the information is readily available on the website.

The second would be the aggregation of more sources and displaying the data from those sources in a precise and concise manner. If I could scrape organizations pages for their events it would thereby automate the “event creating” process. This would mean more events would be available and those seeking these events would have a larger selection. However, this would mean we would have to organize how we displayed the events, so they aren’t all jumbled on the main page. Perhaps creating a search or sorting functionality would be useful. For example, a user might want to look for events within a certain radius or their location, or at a specific time window, and in order to do this we would have to both be able to better collect and display the information.
",,http://redistribute1.github.io,,Best Hack for Social Good (J. P. Morgan),"javascript, google-maps, firebase",1,Johns Hopkins,,,naevinanukornchaikul,Johns Hopkins University,0,,,
Analysis of GEN Encoder,https://hophacks-fall-2019.devpost.com/submissions/126472-analysis-of-gen-encoder,"HophacksNLP

NLP project for HopHacks
",,https://github.com/skyranakis/HophacksNLP,,Winner of Microsoft Marco Challenge (Microsoft),python,1,Johns Hopkins University,,,adalmia96,Johns Hopkins University,1,skyranakis,,
ShareBack,https://hophacks-fall-2019.devpost.com/submissions/126473-shareback,"Inspiration

Ever been asked at checkout to donate to a cause, and hesitate because you're not sure where your money is going?
With ShareBack, your donation stays a credit to the store you're at.
Our partners at local homeless shelters will provide vouchers to those in need.
These members of your community will then redeem the credit you donated for a free meal or item.
It's that simple.

What it does

Our platform allows a small business to become a participating partner by accepting donation credits redeemable at their own location.  Money is money -- there's no difference between money spent at the company through traditional means or via ShareBack credit! More business = happier business!
We then look to partner with local homeless shelters to give out vouchers to those in need redeemable at these locations.  Let's get homeless individuals involved in the community at the same places the donors are.

How I built it

The web app uses QR code tech to provide a proprietary page for each on boarding business.  The monetary allocation is handled securely on the backend.  

Accomplishments that I'm proud of

Check out our site!

What I learned

Design problems don't exist in a vacuum: it's important to evaluate a whole system when developing a robust solution, which is what I aimed to do!

What's next for ShareBack

This is a highly scalable project with large potential for launch in the Baltimore area.  Would be interested in discussing partnerships with local foundations.
",,http://shareback.mystrikingly.com,,"Best Hack for Social Good (J. P. Morgan), Best Hack for a Philanthropic Cause (Bloomberg)",,0,University of Pennsylvania,,,AlexaSpagnola,University of Pennsylvania,0,,,
iWheel-inator,https://hophacks-fall-2019.devpost.com/submissions/126482-iwheel-inator,"iWheel-inator

The Only Disability In Life Is A Bad Attitude

A multiservice detachable device for the elderly and handicapped to solve some of the daily challenges faced by millions worldwide.

Inspiration

We thought of this idea keeping in mind the problems the elderly and disabled face (including our grandparents and relatives) in their everyday lives.

What is iWheel-inator ?

A central promise of technology and all its advancements is to bring a better world through solving inherent problems of human society and its individuals. The predominance of smart devices is so obvious that it is now impossible to imagine a world without them around. However, there appears to be a lack of consideration regarding accessibility in contemporary technology, with the industry being heavily biased towards people without impairments of any kind. Nevertheless, things like a smaller market size and weak representation in government and industry are no longer viable justifications for a pervasive tendency towards the aforementioned specific catering. Apart from non-technical impediments, there are also some major technical challenges in providing these people with high quality services on par with natural human capabilities. iWheel-inator aims to capitalize technology and make a difference is in the realm of physcial impairment. 

Ordinary tasks become difficult and make the disabled elderly less independent. Simple things such as moving around or recognising who they're conversing with become exceedingly difficult. We believe we can address these issues, along with a cheap device we call ‘iWheel-inator’ that we intend to design and build. iWheel-inator is an intelligent digital assistant who can understand requests of its user via voice commands and can provide thoughtful responses through its voice while also performing various functions. iWheel-inator tries to find answers to questions by gathering information from the environment that it sees through a high-quality built-in camera. The concept for iWheel-inator is a simple one, with the aim to make it as seamless as possible to integrate into daily lives. Hence, we want to incorporate the device as an attachment to regular wheelchairs to provide a nice viewpoint for the camera while minimizing impedance with the user’s activity. Essentially, the device is intended to be a third leg for the user and make tasks, simple or complex much easier to deal with while also eliminating the need for smart phones.

Why iWheel-inator ?

People living with disabilities – whether hidden or visible – have hopes, dreams and frustrations just like everyone else. There are many struggles people with disabilities go through. 

Hence, iWheel-inator, acting as a plug and play instrument provides features which help the user:
1) Recognise who's in front of them and whether he/she is known to the user from having met before.
2) Eliminate the need for a smart phone by substituting it with the device itself, say- weather information, reading the news, etc.
3) Read prescriptions and other such illegible documents.
4) Navigate to and through unknown routes without depending on anyone else.

These features were thought of while keeping in mind how forgetful the elderly tend to be and how weak their sense organs like the eyes become with age.

Why is iWheel-inator better?

iWheel-inator empowers users with a distinctive set of features that other competitors are nowhere close to.


It’s all about the unique user experience that iWheel-inator delivers. iWheel-inator is supposed to be like a buddy who is there to support and guide its users. It is a new platform and isn’t comparable to any other existing
solution. 
iWheel-inator is fully voice controlled and hence, with iWheel-inator users can get what they are looking for just by asking for it.
According to our estimations, the cost of iWheel-inator is a lot lower than a high-end device like iPhone. Currently,
seeing AI and such technology is only available on iPhones and such devices. This makes the smart phones less accessible as a solution due to the high cost associated. Thus, iWheel-inator is an excellent alternative to smartphones (not to forget again, how forgetful elders are and how regularly they forget to take their phones- if someone is handicapped, there's no way they'll leave their homes without their wheelchair though).


Challenges we ran into

A few technical difficulties we faced were integrating the Google Maps Direction API as well as implementing the facial recognition technology into our project. We were also unable to find appropriate hardware to demo the product as a wheelchair.

Accomplishments that we're proud of

We delivered what we had planned to implement and that's something we pride ourselves in.

What we learned

Each one of us had to learn something or the other as we divided our work into modules and worked upon each feature separately. We learnt how to use GCP, Google Maps Direction API, etc.

What's next for iWheel-inator

We intend to add more features, maintain and improve the existing features and User Experience in the near future. Once this is done, we hope to turn iWheel-inator into a well-established startup.

Want to know our business model?

iWheel-inator is a plug and play device as mentioned before and hence the major part of ""our"" expenditure- the actual wheelchair cost is cut down. 

We plan to give tablets (acting as display screens) to our users which would be integrated into the exisiting wheelchair by our technical assistants. 

Keeping this in mind, we expect the cost of product to be around $100 and the selling price to be around $130 (additional $1 per month for database costs and improvements to the product).

How we built it

An image describing our Tech Stack has been uploaded.
",https://youtu.be/xxMiLAgqODs,https://github.com/akshatvg/iWheel-inator,,"Best Domain Name from Domain.com, Best Healthcare Hack (Siemens), Best use of Google Cloud, Best Hack for Social Good (J. P. Morgan), Best Hack for a Philanthropic Cause (Bloomberg), Best Hack for Mental and Physical Well-being (Accenture)","flask, html5, css3, javascript, bootstrap, domain.com, gcp, google-cloud, ibm-watson, amazon-rekognition, google-directions, google-maps, amazon-textract, amazon-web-services, azure, nexmo",1,"Vellore Institute of Technology, Vellore",,Domain.com|-|Google Cloud,akshatvg,"Vellore Institute of Technology, Vellore , Vellore Institute of Technology",2,unknown-guy-1610,wwwanandsuresh,
Scribe,https://hophacks-fall-2019.devpost.com/submissions/126511-scribe,"Inspiration

Going into doctor offices I really feel that due to documentation that is required the personal aspect of medical professional and patient conversation is lost. We found a need where the visually impaired can use this software to fill out forms.

What it does

By connecting to a database of questions, based on the patient needs/visit questions the doctors need to ask will be listed. When in the office the conversation is captured and based on keywords the program puts into categories of medication, patient vitals, and patient condition. After the visit is done a document is created and the prescription is filled automatically.  

How I built it

Using google cloud, google speech api, node.js for the web, and native android

Challenges I ran into

Making a clean UI, and create a better algo to process the categories

Accomplishments that I'm proud of

applying speech recognition in a real application.

What I learned

google speech api, node.js

What's next for Scribe

Create a patient side app where medical forms can be filled before visiting the clinic.
",,https://github.com/emkaysri/hophacks,,"Best Healthcare Hack (Siemens), Best use of Google Cloud, Winner of FastForward U (FFU), Best Hack for Social Good (J. P. Morgan), Best Hack for Mental and Physical Well-being (Accenture)","java, kotlin, html, shell",1,"University of Wisconsin - Madison, Johns Hopkins, Rutgers",Arduino 101|-|Dell XPS|-|Google Home,Google Cloud,emkaysri,,0,,,
noteshift,https://hophacks-fall-2019.devpost.com/submissions/126516-noteshift,"Inspiration

Transposing allows you to play music meant for another instrument. It's a very useful skill to learn as you are often put in situations in which you want to play a song in another key or semitone. However, transposing is also very tedious and can take hours to manually do (sometimes taking hours). Though a few transposing solutions do exist, they require you to manually input each note, making the process very slow. Thus, we created NoteShift to automatically transpose music for anyone who has access to a computer or smartphone. We hope that our software will make the lives of all musicians easier.

What it does

Our app will allow the user to submit music through our website and signify which key the music is in and which key they want to transpose to. Based on the user's input, NoteShift automatically transposes the music and uploads a downloadable PDF of the new music. The user will also have an option to hear the original song and the transposed song.

How it works

When a user clicks submit, the browser redirects to a script that first appends key information to the front of the file name, then uploads the entire file to the server. The browser then redirects to a loading page. In the backend, a new file is detected by a daemon script and triggers a series of scripts. The pdf is first converted into MusicXML using the OMR Machine Learning library Audiveris. A Java program then transposes this file. Finally, a music notation program called MuseScore converts the transposed MusicXML file to both pdf and mp3 versions.

The backend creates a file, which is detected by javascript running on the loading page. Javascript then redirects the client page to the result, displaying both the transposed pdf and mp3 on the client's browser. Processing takes about 30 seconds per processed page.

We used a DigitalOcean server to run computations.

How we built it

We split up into teams to manage different parts of the coding process. Xiangyu and Lawrence worked on the UI/UX and communication from and to the front end. Ricky worked on the algorithm to transpose music to different keys. Tyler managed the DigitalOcean server and created the daemon to run the whole process.

Challenges we ran into

Because of the details and quirks of music notation, transposing music isn't as simple as ""incrementing x number of semitones"" to a note. Enharmonic notes must be transposed correctly to the new key. Our algorithm to do this transposition now correctly takes notes to the correct octave and note-names but struggles with putting the correct
accidentals on the music.

Being unfamiliar with web development before starting this project, we ran into many issues trying to communicate between the front and back end. This occupied a good chunk of our time, though we were eventually able to find solutions to our communication need.

Accomplishments that we're proud of

We have a working app! An MVP of our product was successfully hacked together and can transpose music in the keys of [C, Eb, Bb, F, G, A] to music in the keys [C, Eb, Bb, F, G, A]. We were also able to implement a way for the user to playback their old music and their transposed music.

What we learned

We learned roughly how to communicate between the front end and back end through HTML requests. We also learned how to work with the MusicXML file format.

What's next for NoteShift

We want to support image formats (.jpeg, .png) so that anyone can casually take a picture of music and transpose it. OpenCV has image editing algorithms to apply the necessary thresholding and perspective transform. A demo will be provided.

We want to support different methods of inputting the desired transposition. Our site is geared towards band music at the moment, and making it more friendly to people that just want to transpose to an easy key is high on our list of changes.
",,http://noteshift.tech,,"Best Domain Name from Domain.com, Winner of FastForward U (FFU)","html, php, css, shell, javascript, bash, musescore, audiveris, apache, java",1,Johns Hopkins University,,,ShadowX74,Johns Hopkins University,3,DenouementX,shintyl,rickyycheng
TerpV-U,https://hophacks-fall-2019.devpost.com/submissions/126524-terpv-u,"TerpV'U

Inspiration/Problem to Solve

Whether you're a Public Speaker, Professor, or even a Pastor, there has always been moments where you've seen people dose off on your their smartphones (you might be one yourself). Excessive phone usage can be an indication of a boring speech, an uninterested crowd of students, etc. Wouldn't it be nice to be able to quantitatively visualize how many people were on their phone and for how long? 

How?

Current, smartphone/phone detection Vision AI models can only be used from a relatively short distance. Thus, the capability of detecting the number of phone in a large lecture hall or conference room is virtually impossible with this approach. However, smartphone screens emit a small amount of Inferred light (IR), which we can take advantage of. By modifying a webcam to only let through inferred light, we can create a simple yet effective vision model that detects and tracks the IR light emitted by smartphones. This can easily be implemented large scale by placing the camera above and slightly behind the audience. 

Technology

We decided to utilize a simple Logitech webcamera for the hardware. As for the software development side, we used Flask (a Python web framework), OpenCV (a computer vision library), and Bootstrap (a CSS framework). 

Implementation
",https://youtu.be/TeHqUo-x5sw,https://github.com/peterchun2000/TerpV-U,,"Best Domain Name from Domain.com, Winner of FastForward U (FFU), Best Hack for Social Good (J. P. Morgan), Best Hack for Mental and Physical Well-being (Accenture)","python, c, c++, html, tex, javascript, jupyter-notebook, fortran, css, shell, matlab, powershell, makefile",0,University Of Maryland-College Park,,Domain.com,dakshaymehta,"University of Maryland - College Park, University of Maryland - University College",3,peterchun2000,ahwishel,andychan33
Overachiever,https://hophacks-fall-2019.devpost.com/submissions/126560-overachiever,"Inspiration

Many students at Hopkins often wonder if they should add a certain major or minor, but are concerned if it will be too many extra courses or not. This will help them

What it does

A Hopkins student enters the courses they've taken so far, and it will give me the top three minors that they can get without having to take too many more courses

How we built it

The backend and logic is built in python. We use tkinter for the gui. JHU courses are pulled in using the SIS API.

Challenges we ran into

Trying to get the gui to work

Accomplishments that we're proud of

Finishing

What we learned

As CS majors, it'd be easy for us to all get CS minors!

What's next for Overachiever

Implement more minors and majors, move to web app
",,https://github.com/Anderson-A/HopHacksFa19,,,"python, tkinter",1,Johns Hopkins University,,,Anderson-A,Johns Hopkins University,0,,,
StreetSmart,https://hophacks-fall-2019.devpost.com/submissions/126596-streetsmart,"Inspiration

We were inspired by Braess's Paradox - the phenmenon that adding a road segment to a network can actually increase overall travel times, because of the tragedy of the commons. It follows that deleting certain road segments can decrease overall travel time. This phenomenon has been repeatedly observed in the world, such as with the Cheonggyecheon Bridge in Seoul, South Korea, the road network in Stuttgart, Germany, or 42nd Street in New York City.

The benefits of doing so are varied. Americans spent 8.8 billion hours stuck in traffic in 2017, for a total financial cost to the nation of $166 billion. The trends are only getting worse; by 2020 the costs are projected to rise to $200 billion. Baltimoreans suffer particularly due to traffic congestion; our city ranks 22nd-highest in amount of time motorists spend in traffic. The cost to each motorist is 59 hours, 22 gallons of gasoline, and $960 wasted per year.

Innovative and novel solutions are needed. Braess's Paradox results in inefficiencies that can up to double overall travel time in a network, so isolating and removing roads that are contributing to congestion can help alleviate traffic congestion. These road segments can be converted to pedestrian-friendly streets and plazas, which can make neighborhoods less congested, more small-business friendly, and more human-centric!

What it does

StreetSmart is a tool to identify real-world streets like the one in Braess's Paradox, which would improve overall traffic conditions if removed. 

How we built it

We began by finding open source road map data that was query-able by address.  We decided that OSMnx--a Python package for interfacing with Open Street Maps--was the best way to conduct our queries and eventually build our dynamic overlay maps.  Next, it was necessary to construct a reasonable model for traffic. We reviewed resources on traffic modeling, formulated the problem, and began experimenting with Python.

Once we had a working traffic model with actionable results, the next task was to deploy our tool to the Web. Luckily, with the help of Google Cloud, we were able to set up a virtual machine to run our service.

Challenges we ran into

We were presented with three major challenges:


Coming up with a realistic model. We applied intuitive principles (""conservation of cars"") to make sure our model reflected the theory and reality of cities. 
Optimizing our algorithms to enable real-time results. We rewrote a large portion of our Python code in C++, achieving 100x speedups.
Overcoming deficiencies in libraries. For instance, the library OSMnx had limited interoperability with Folium. We had to modify some functions to enable road coloring.


Accomplishments that we're proud of

We are proud to have created an end-to-end solution that works. To our knowledge, this is the first open traffic modelling tool specifically aimed at addressing Braess's paradox. We hope our tool can help cities transition towards more pedestrian-friendly, community-centered, social layouts.

What we learned


Overlap between computing problems and social problems
C++ is much faster than Python
How to use Google Cloud to host a web service


What's next for StreetSmart

StreetSmart relies on OSMnx to interact with Open Street Maps. However, OSMnx uses a graph middleman package: NetworkX.  NetworkX provides a lot of unnecessary functionality and makes the final steps of our graph processing slow for large areas. We would hope to reduce wait time for generating dynamic maps by implementing our own graph library, likely in a lower level language.
",,,,"Best use of Google Cloud, Best Hack for Social Good (J. P. Morgan), Best Hack for a Philanthropic Cause (Bloomberg), Best Hack for Mental and Physical Well-being (Accenture)","python, c++, osmnx, flask, folium, google-cloud, openstreetmap, html, css",0,"Johns Hopkins University, Stanford University",,Google Cloud,nnaparst,"Johns Hopkins University, Stanford University",3,ccosgrove,EricZelikman,karosterbauer
USleepin?,https://hophacks-fall-2019.devpost.com/submissions/126598-usleepin,"Inspiration and what it does

""USleepin?"" is a Google Chrome Extension inspired by a research study conducted by the Imperial College of London, which showed the impact of sleep deprivation on physical reaction time, working memory, and productivity. In this extension, users have the ability to select time intervals between 15 minutes to 1 hour. Each time that the user's selected time interval elapses, a popup will appear and direct the user to an interactive response stimulus which records their reaction times. After playing the response game, users can view the analytics and history of their response time plotted on a graph to visualize the effects of sleep deprivation on their alertness. The anticipated audience of this app would be college students who regularly or occasionally need to stay up to complete assignments or study for exams. It can also be applied to individuals who practice unhealthy sleeping habits because the app can quantify how their productivity and work is negatively impacted by their tiredness.

The purpose of this app is to allow users to identify when their alertness is no longer at peak performance and they should really, as we say, ""go to sleep ya fool!""

How we built it

Overall design features a simplistic chrome extension plugin that utilizes javascript components within an html body that enables pop-ups that redirect the user to the alertness test at specific intervals of time that the user specifies through buttons within the chrome extension pop-up. After the test, the page redirects you to an Analytics page, where a plot of a user's general alertness is featured, as well as our story and introduction. We regularly committed and pushed our files to Github. We linked all of our files through the html script and src tags via buttons and functions we wrote in javascript. We wrote a basic interval-based repeating timer, as well as our own 'game' where the image disappears and reappears randomly throughout a canvas for a set period of time. We store our data on the web app.

Most PNG images were designed on Adobe Illustrator with a Intuos drawing tablet. Gifs were created as frame animations generated with Photoshop.

Challenges

As a team, we ran into a variety of issues. In particular, working with javascript and HTML were difficult because these languages are often finicky, and most of our team members had little to no experience coding in javascript, which is the code that was predominantly used in the final product. In order to maintain the authenticity of the app, we wanted to design most of our own graphics, which required vector images. Consequently, we experienced a couple mishaps with adobe illustrator, including the loss of a graphic that we spent hours on.
Perhaps the most challenging part of the project was actually brainstorming ideas for the app. While we wanted to develop something impressive, we also wanted to make sure the task was accomplish-able with our given skill sets. Naturally, as the hours dragged on, fatigue became an issue that we had to overcome and working through coding errors and graphic design on minimum sleep resulted in moments of severe frustration and lots of ""I don't know what I'm doing!!!!!!"" Though it certainly felt like we were part of the Great British Bake Off (but with code instead of batter) we certainly had a good time and felt reassured by the communication and camaraderie of our group.

Accomplishments that I'm proud of

We are definitely proud of our ability to work as a team. We collaborated and communicated effectively and we all approached the Hackathon with a positive mindset, high spirits, and determination. We also played to each of our strengths which allowed us to not only produce a relatively polished finished product, but to also learn new things about the techniques and languages we were already familiar with. We are proud to have created an awesome new chrome extension with tons of exciting new ideas that are ready to be expanded upon. 

What I learned

Within a span of 36 hours, we learned how to code in javascript, manipulate adobe illustrator more efficiently, generate vector images, and work with HTML and CSS styling.

What's next for USleepin?

We have lots of exciting new things in store for ""USleepin?"" We hope to: make the game longer to better test the reaction speeds and attention spans of our users, save the history of past reaction speed attempts, make a leaderboard showcasing user's top reaction speed times, improve the alert system of our extension by making alerts appear at the bottom of page, change the time buttons to sliders ( so there is more flexibility to specify desired time intervals), generate a page that yells at the user to GO SLEEP when the user's reaction speed falls below an average threshold and add a larger variety of games to keep users on their toes.
Future applications of ""USleepin?"" include: sharing the app with the student body so that they can be more conscience of their sleep health, publicizing the app as an accessible tool for sleep and cognitive function research, and adding more sheep gifs to maximize cute factor.
",https://youtu.be/M6yGPK2aKvw,https://github.com/Reemeela2121/yousleepin.git,,"Best Healthcare Hack (Siemens), Best Hack for Social Good (J. P. Morgan), Best Hack for a Philanthropic Cause (Bloomberg), Best Hack for Mental and Physical Well-being (Accenture)","html5, css3, javascript, json, gif, png, vector",1,Johns Hopkins University,,,Reemeela2121,Johns Hopkins University,3,slu29,ntucai1,avulaj1
Discourse,https://hophacks-fall-2019.devpost.com/submissions/126609-discourse,"Inspiration

Discussing a topic with others is one of the best ways to demonstrate knowledge about a subject, and to fix it where it is lacking. With careful engineering, computers should be able to take part of that conversation too.

What it does

We built a voice-based study tool that engages users with a set of questions about a topic - to which they respond in natural language. Our tool then evaluates their responses and points out what might be missing.

How we built it

We used Google's Dialogflow to handle intent inference and the flow of conversation. We then deployed the agent onto Google Assistant, which is available on mobile devices. Questions and evaluations are routed to the agent from a python webserver, which uses Facebook's Fasttext word embeddings to score the quality of responses and note where there are errors.

Challenges we ran into

None of us have experience with NLP, so understanding what a good approach was, and then navigating the set of available tools was difficult, and we fully implemented two solutions with poor results before finally settling on Facebook's Fasttext + a custom scoring algorithm.
In addition, three of us were not very comfortable with python, and none of us knew much about Dialogflow. There was a lot of learning involved on all sides!

Accomplishments that we're proud of


Learning about NLP, and implementing our first NLP solution
Making an actual application that works and that someone can use.


What we learned


A crash course on word embeddings
Dialogflow
Python
The value of having fun - but we already knew that


What's next for Discourse

Writing a more comprehensive set of questions and answers to inform how we evaluate and modify our approach. It would be best to collaborate with an education company in a specific domain to fine-tune our ideas. It is also important that we polish the interface, maybe as part of our own app.
",,https://github.com/stewy33/hophacks2019,,Best use of Google Cloud,"dialogflow, google-web-speech-api",1,Johns Hopkins,,Google Cloud,stewy33,Johns Hopkins University,1,harshild24,,
Dorothy ,https://hophacks-fall-2019.devpost.com/submissions/126610-dorothy,"Dorothy - Smart Therapist, Caring Friend in Your Pocket

Inspiration

Mental health should not be an issue that is ignored. Therapy should be something that you don't loathe, or avoid. We know that mental health treatment is an important part of a child’s healing and that addressing the impact of trauma on the child significantly reduces harm and decreases the risk for future abuse. Therapy can help a child work through difficult, confusing and painful feelings in a 
safe setting. Therapy also provides children with the tools for going forward and
leading healthy, productive lives. This is why we developed Dorothy.

What it does

Dorothy is AI-powered friend with intuitive understanding of young people's well-being, especially the youth. Dorothy is able to detect real-time emotion from tone of voice, facial recognition and sentimental analysis of text. Since Dorothy is geared towards adolescent, we make the app to be more interactive and engaging by providing features from music suggestion to self-guided meditation. We also add feature to locate the closest and most needed help resource based on the sentiment analysis and NLP

For young working adult, Dorothy uses NLP to be that hearing ears in a lonely night in New York City.

How we built it (with brain)

User-Interface

We start with a focus on intuitive and eye-catching UI/UX. To do so, we use Flask Framework, Javascript, Html and Css.

Speech-to-text

Next, we brainstorm what is the best approach to a quite challenging topic. Understanding that most adolescent feels more comfortable express themselves through spoken words than text, we decided to first work on the speech-to-text feature. To do so, we use Microsoft Azure Speech Recognition Api and flask to properly display the output.

Text Sentimental-Analysis

To do so, we use TextBlob library in python. Dorothy is able to differentiate emotion based polarity and subjectivity.

Emotion Facial Recognition

We use OpenCv to have a real-time emotion detection. For Facial recognition, we use Tensorflow Emotion Recognition Face API. Dorothy is then able to enhance her emotion recognition features to encompass more emotion, such as Fear, Disgusting, Angry, Neutral, Happy, Surprise.

Natural Language Processing Help-Bot

We train a bot on Cornell movie dataset to be able to understand the english semantic as well as answer any question in the most natural way. Dorothy then incorporates Text Sentimental-Analysis and Emotion Facial Recognition to help user either talk their feeling out or guide them to the next proper step. 

Brain and Psychology Domain Knowledge

Dorothy is built on the foundation of Cognitive Behavioral Therapy, Psychotherapist and Human Development understanding!

Challenges we ran into

Well. There were a lot. First one is probably getting lost in John Hopkins University.

Second is to find optimal solution that incorporates Cognitive Behavioral Therapy as well as Artificial Intelligence to solve a challenging problem.

Regarding technical challenges, there are some aspects we are less proficient than other but we were able to put every piece together efficiently!

Accomplishments that we're proud of

Probably most of the project. This is an idea that we have for a long time and finally we bring into fruition. However, we are most proud of the fact that we help solve a problem that we are both concern about!

What we learned

Sleep is optional!
Other than that, I (Hang) feel like I gain +1 level in JavaScript and Flask proficiency.

What's next for Dorothy - Your Digital Friend

WOW! we have a lot more in store for Dorothy.


Have a MVP for iOS and Android app
I (Hang) am personally super excited to add in Tone Emotion Detection. That is why we initially come up with the idea to do speech-to-text feature. Tone of voice shows a wider range of emotion. Then, we will enhance Dorothy's ability to understand emotion further.
We would love to take the NLP further to be more suitable for the audience. Using a bigger and more appropriate dataset will help Dorothy to sound professional and caring like a true friend.
Finally, we want to create a wellness report for the user based on the conversation. For example, if there is a trend in their mood getting better and what is the reason for such. Ultimately, I am really interested in the collecting a dataset that helps researcher and scientist understand the early sign of depression in adolescent!

",,https://github.com/rgangu/dorothy,,"Best Healthcare Hack (Siemens), Best Hack for Social Good (J. P. Morgan), Best Hack for a Philanthropic Cause (Bloomberg), Best Hack for Mental and Physical Well-being (Accenture)","flask, neural-networks, tensorflow, keras, opencv, tensorflow.js, javascript, python, azure, google-cloud, natural-language-processing, html5, css3, textblob",1,Penn State,,,rgangu,Pennsylvania State University,1,Hangpanbee,,
HiJean: Helping with basic needs,https://hophacks-fall-2019.devpost.com/submissions/126630-hijean-helping-with-basic-needs,"Situation: During the sponsor talks, it was brought up that people in poorer communities would choose hygiene products over food. Hence this brought the idea of a donation application to link individuals who are willing to help others and those in need.

The application displays people in need in the area, with a list of items (such as toiletry and clothing) they may need. Once they find something they lack, they contact the provider for a transaction meetup. After the exchange takes place, the user is reward with a karma system based on the type of items donated, and the frequency of giving.

We hope to create a stronger link between those in need and those who want to help through this mobile application, since the communities in need report using their phones as a mean to the internet.
",,https://github.com/xapatjb4/HopHack19,,"Best Healthcare Hack (Siemens), Best Hack for Social Good (J. P. Morgan), Best Hack for a Philanthropic Cause (Bloomberg), Best Hack for Mental and Physical Well-being (Accenture)","javascript, android, ios, flutter, node.js",1,UMass Dartmouth,,,xapatjb4,University of Massachusetts - Dartmouth,1,agrozdani,,
SafeJourney,https://hophacks-fall-2019.devpost.com/submissions/126631-safejourney,"What is our project?
The purpose of SafeJourney is to provide its users with a navigation system
that factors police crime data into its algorithm to navigate away from
dangerous streets. It maps the shortest and safest route from point A to
point B. Police reports update the algorithm with current crime information
to constantly adjust and improve its pathing.

Why our project?
We want our project to help Baltimore residents stay safe in their day to
day life. Kids can walk to school on safer paths.

USAGE
Currently, this application is based entirely within your terminal. Compile
every .java file. Run StreetSearch with the following arguments: mapfile,
start Latitude,Longitude pair, and end Latitude,Longitude pair.

example:
$ java graphFiles.StreetSearch graphFiles/baltimore.txt -76.6063,39.2907
-76.6136,39.3195

FUTURE GOALS
-Planning to build app with gui
-updating info and filtering it based on type of crime more thoroughly
-being more precise/predictive in the severity of danger in various areas
-give more detailed directions with a focus on accessibility for all users
-Implement more efficient A* shortest path algorithm
",,http://github.com/dskaff/hophacksfall2019,,"Winner of FastForward U (FFU), Best Hack for Social Good (J. P. Morgan), Best Hack for a Philanthropic Cause (Bloomberg)",java,1,Johns Hopkins University,,,jmkurlander,Johns Hopkins University,2,dskaff1,emilyzeng328,
Cross Lingual Information Extraction for your everyday needs,https://hophacks-fall-2019.devpost.com/submissions/126643-cross-lingual-information-extraction-for-your-everyday-needs,"Inspiration

We're motivated by the necessity to remove linguistic barriers to information and enable access to both sides of a story.

What it does

It lets you search the web in a language you know(eg. English ) for information that might be present in a foreign tongue. This project is a result of labour of love. 

How we built it

We translate your QUERY into languages of your choice. We then search the internet for both your query and the Translated Queries.  The results of the queries are then RANKED using algorithms trained on MS Marco reranking. We then take the top 5 resulting pages and using our QUESTION ANSWERING system to extract an answer to your searched Query instead of just showing you a list of pages.

Challenges we ran into

A major challenge we faced was handling a high volume of translation traffic. 

Accomplishments that we're proud of

A high quality search engine that gives more contextual, appropriate search results for regional, language specific search queries.

What we learned

Leveraging API calls for automated search and translation, question answering using SOTA models.

What's next for cross-lingual-QA-MSMARCO

Auto-detect language of query and translation.
",,https://github.com/abhinonymous/cross-lingual-QA-MSMARCO,,,"python, shell",1,johns hopkins university,,,abhinonymous,Johns Hopkins University,1,aniruddha16293,,
Math Pix PDF Full Scan,https://hophacks-fall-2019.devpost.com/submissions/126665-math-pix-pdf-full-scan,"Inspiration

I was amazed when I saw how accurate and quick Mathpix was during the opening presentation. It caught my eye because of the possibilities it could lead to, and building on top of the API today showed me how great the potential is for this technology. 

What it does

Mathpix is a great piece of software that is able to detect text AND handwriting and turn it into LaTex, a mark up language, where it is then printed on to the screen for digital use. What I created was a program that took the Mathpix API and made it so instead of being able to only parse a couple lines of writing, it is now able to scan through a whole PDF page and convert that into LaTex. No more having to manually partition the photos yourself.

How I built it

The program was built with Python, and utilized many python libraries such as json, pdf2image, PIL, and more.

Challenges I ran into

I had no knowledge of what LaTex was going into this, and minimal idea of how to even work with JSON and APIs. I was also in a team by myself, so I was short on manpower, data processing, and skills.

Accomplishments that I'm proud of

I am extremely proud of being able to learn so much about JSON, APIs, and LaTex, but most of all I am happy that I was able to create an application that satisfied the hacking criteria all by myself.

What I learned

Aside from the bigger pieces of code that I mentioned earlier, the biggest take away from this project was reading and understanding library documents and sample code. Without these I would have been no where.

What's next for Math Pix PDF Full Scan

I really wanted to incorporate OpenCV OCR and have the text be partitioned in a more precise and mathematical way, but it was out of my scope at the moment. With more time I would add functions that reduce the time complexity because the program is just a bit too slow for my liking. 
",,https://github.com/chuefeng/HopHacks19,,,"python, json, os, slice, api, snipnotes",1,UMass Dartmouth,,,ChuefengVang,University of Massachusetts - Dartmouth,0,,,
MUSETTE,https://hophacks-fall-2019.devpost.com/submissions/126684-musette,"We believe that music can heal, and through our app, we strive to use technology to bridge the gap between those who can provide musical therapy, and those who might benefit from it, especially children with special needs. We are using music as a form of alternate medicine.

Our inspiration

As we were researching on a topic for our app, the subject of Music Therapy caught all of our eyes. We were fascinated by the magical healing power of music for patients with mental disordered and brain traumas- at that moment, we knew that Music Therapy will be the topic of our app.

As we dived deeper into the world of Music Therapy and mental diseases, we were stunned by the statistics that 1 in 160 children have Autism Spectrum Disorder (ASD) while 5.29% have Attention-Deficit/ Hyperactivity Disorder (ADHD) globally. The National Institute of Mental Health estimates that 3-5% of children have ADHD, or approximately 2 million children in the United States. In 2018, the Centers for Diseases Control and Prevention determined that about 1 in 59 children is diagnosed with ASD.

Biologically, metal illnesses tend to be associated with abnormal levels of neurotransmitters, like serotonin or dopamine in the brain, a decrease in the size of some areas of the brain, as well as increased activity in other areas of the brain. Research shows that pleasurable music increases dopamine levels in the brain.

What does the app do

Our mobile application, MUSETTE, provides soundtracks and animations that acts as follow-along guides for patients to carry out activities. Our application intends to help children with special needs in the following ways:

Practical methods
For children with ASD/ ADHD, early intervention cultivate their social skills and strong family bond. These factors are important to the development of the SEN children. However, there is a lack of support service for them. Other than community support services and professional help, parents, teachers, and social workers may not know what else they could do to allow SEN children to reach their full potentials. MUSETTE provides practical ways for caregivers of SEN children to aid the healthy growth and development of the children by cooperating with music therapists in engaging in the music therapy activities outside of therapy sessions.

Financially affordable 
In contrast with the relatively expensive therapy sessions held by music therapist, MUSETTE is a free app that allows parents to engage their children in music and activities designed by music therapists, helping the children retain what they have learned at the therapy sessions, and even make progress. Moreover, caregivers can rate the performance of children, which helps music therapists assess the overall improvements of the children. These could increase the overall effectiveness of music therapy, and after the initial period, reduce therapy sessions and thus, alleviate the financial burden from families.

Improve relationships between caregivers and SEN children
Children with ASD/ ADHD may find it difficult communicating their feelings and thoughts, as well as understanding other people. Through engaging in the music activities, caregivers and their children can communicate through specially designed activities, which helps foster better relationships.

Connecting stakeholders
Many people are involved in the growth of SEN children, their parents, therapists, school teachers and social workers. It is important for all these stakeholders to be informed of the latest progress of the children. This can increase the efficiency of the overall treatment, allowing professionals to adjust treatments to suit the children's’ condition. 

Description of the app

First time user may sign up using their google account. Users may identify their role among our options provided, namely: 
Kid Family (for parents)
Music Therapist
Social Worker
Teacher
Others

Information related to selected roles will be asked, For example, name, age group, phone number will be asked for music therapists; kid’s name, age and sex will be asked for kid family. In case the user has more than one child using the app, each child will have to use a different Gmail account.

Users are able to change their profile or logout in the ""Profile"" page after signing in.

There are four main features of MUSETTE:

1. Activities
A list of music (with activities) suggested or designed by music therapists are displayed. Easy to follow animation and instructions enable caregivers to guide their children. Entertaining animation and videos can attract the attention of the child.

For the two activities designed by music therapists, animations and lyrics will be played in time with the music. They can also choose listening to the soundtrack with or without vocal accompaniment. The speed of the music can be adjusted for non-vocal soundtracks in order to cater for the needs of every children. Instructions of sing-along activity will be provided. The aims for each activity are included in the instructions (e.g. improve the attention span of child). Users can choose to show a white screen to avoid distracting the child. 

For other music, users will be directed to a YouTube video. They can play the video and follow along the instructions provided in the videos.

After the activity, caregivers can choose the date of the activity and rate the performance of the child therapy sessions by giving stars.  

If the users have multiple connected children (e.g. music therapists, teachers, social workers), they can select the children whom they are carrying out the activity with (from the list of connected students) and input individual ratings.

2. Connect
Data sharing between stakeholders enables music therapists to assess performance of child more accurately, increasing overall effectiveness of music therapy on the child.

Under the ""Our community"" section, caregivers can connect other users through inputting or scanning their personal QR code (Kid Family users can only connect with non-Kid Family users and vice versa). Connected users will be shown in under this section. Users can remove connected users by tapping onto the name of the connected users and press ‘disconnect’. 

Under the ""Music therapists"" section, a list of professional music therapists in Maryland (whom we found online) is displayed here. Their emails, phone numbers, and credentials are shown for other users to search for when they wish to seek assistance from professionals.

3. Statistics
This page shows previous records of the selected child's performance (in a week, a month, 3 months, or 6 months), which enables music therapists and caregivers to proactively address strengths and weaknesses of the child, as well as measure progress effectively .

4. Resources
This page provides credible information from professional music therapist point of view to users. 

Under the ""Helpful articles"" section, information related to children with special needs will be provided here. Our team hope to consult music therapists and regularly update the most recent and useful information. 

Under the ""Other activities"" section, a list of activities related to music therapy that can be conducted without provided music, such as requiring children to compose a melody, is shown here.

How we built it

We used MIT App Inventor 2, a cloud-based app development tool to build MUSETTE. Data such as user information, performance of children, song lists, and article lists are stored in Google Fusiontables. Google Fusiontables API is used such that users can access these data on the app.

Whats Next for MUSETTE?

1. Cooperating with music therapists to launch a diagnostic test for users
It has been observed that even though music has healing properties, some children might also prefer silence to increase focus. Music has different effects on children as their needs and preferences vary. Therefore, we believe that the right way to get started with music therapy is to get a ‘Musical Diagnosis’. We hope to partner with music therapists to curate a form that allows users or parents to fill in information about their children, the symptoms and anything that they have particularly noticed which may be of help. Based on the entries to the form, our app can suggest them special playlists to get them started on their musical journey. Alternatively, they can also schedule one-on-one appointments with the therapists to talk to them about their children and their needs.

2. Adding more songs for users to purchase
We hope to collaborate with music therapists to launch more music activities for a larger range of mental disorders.

3. Create a version for iOS devices
Currently our app is programmed using AppInventor, and is for android devices only. In near future, we also hope to develop our app using Unity for iOS devices, such that the app can reach more people in need.

Challenges we ran into

Our team consists of three first-time participants of a Hackathon, which makes finding the right direction for the app a big challenge. We had difficulty in estimating the time needed to complete an app and logistics of HopHacks, but we are glad that we manage to finish the app and all the submission documents on time. 

Accomplishments that we're proud of

Each of our team member has a different programming skill set: one in java, one in C++, and one in app inventor. We acknowledged the fact that we cannot work on the coding part of the app together from the start. However, our division of labor allowed each of us to perfectly complete our parts, and deliver the best version of our project when all efforts are combined.

What we learned

We have learned that cooperation is the key to accomplishment. In such a limited time, no one person can finish all the work by himself/ herself. We learned that the appropriate division of labor is crucial for the success of a project, as only by doing so, each member of the team can concentrate on doing what they are most skilled at, so as to produce the most desirable product. With an open mind during cooperation, we have also learned to respect each other's opinions. We became more active in voicing out our views when we are in conflicts, which allowed us to take others' comments into our perspective and improve our insights as a whole.
We have also learned the importance of persistence- without it, we would have been defeated by the temptation of sleep and would not have been able to finish this project.
",https://vimeo.com/user102890724/review/360085505/72a487f82c,https://drive.google.com/file/d/1MSDSOQE6i0pOdBgunkVIJVJofRQTfC-Z/view?usp=sharing,,"Best Healthcare Hack (Siemens), Best Hack for Social Good (J. P. Morgan), Best Hack for a Philanthropic Cause (Bloomberg), Best Hack for Mental and Physical Well-being (Accenture)","mit-app-inventor, google-fusion-tables, google-fusiontable",0,Johns Hopkins University,,,looivivian,Johns Hopkins University,1,swadhwa5,,
MoldAI,https://hophacks-fall-2019.devpost.com/submissions/126703-moldai,"Inspiration

I constantly think about how computer vision could help people. My original plan was to train a convolutional neural network to detect skin diseases  and connect it to mobile app. However, that's been done by teams with much more resources, expertise, and experience. Furthermore, the cost of misclassifying instances of skin cancer is very high.

Later I thought that doing the same with mold on household surfaces would help people who lack the financial resources to hire a mold inspector. Also, the cost of misclassification would be low as it would not harm the user. 

What it does

MoldAI classifies images of mold on household surfaces and displays information about it.

How I built it

I used google's image search api to collect images of various species of mold on walls and other surfaces. I manually went though the data to discard irrelevant results and selected the most common strains. 

For the machine learning part I trained a pytorch model to detect the 7 most common strains using transfer learning. To do this I used features extracted by a pytorch implementation of EfficientNet (https://github.com/lukemelas/EfficientNet-PyTorch).

Once I had the model I put it in a python flask server which takes image data submitted through an html form and feeds it into a classifier. The classifier then outputs a list of predictions which are sent to the user along with some information about the detected strains.

Challenges I ran into

Getting the necessary data to train a machine learning model was a major challenge. Finding a neural network architecture that could reliably output predictions with so little data was also a challenge. Creating a frontend was especially difficult given my lack of experience and time constraints.

Accomplishments that I'm proud of

I am very proud that I managed to get 71% accuracy on my validation set considering how small my data set was (311 images). I am also pretty stoked that I managed to connect my machine learning model to a usable frontend.

What I learned

I learned how to do transfer learning with EfficientNet and how to put a machine learning model into a usable application.

What's next for MoldAI

I will collect more data to train a better model and create a frontend in React.
",,https://github.com/vladthesav/MoldAI,,,"flask, pytorch, html5, google",1,Rutgers University - New Brunswick,,,vladthesav,"Rutgers, The State University of New Jersey",0,,,
Spam Slayer,https://hophacks-fall-2019.devpost.com/submissions/126735-spam-slayer,"Inspiration

User review is one of the most important factors that customers want to use when they shop online. However, the authenticity of the reviews is not guaranteed. Some of the reviews are generated by machine. The group members used Amazon a lot. It turns out that humans hardly succeed when we try to distinguish between truthful and computer generated reviews (with a 40% accuracy). Then can machine learn it?

What it does

This application accepts the URL for an Amazon product and filters out the reviews it suspects to be fake/computer generated. A new rating for the product is also generated.

How we built it

The website is built with dJango while the amazon reviews were crawled using scrapy. The reviews are analyzed by a CNN model trained on over 20k Amazon reviews.

Challenges I ran into

We tried to integrated many features in a short amount of time and we unable to perfect each feature as desired. We especially had problems with using scrapy as it was not performing as expected from its documentation. Further, getting dJango to perform fast enough for practical use was a big issue. 

What's next for Spam Slayer

Optimize the speed of dJango and expand to other websites.
",https://www.youtube.com/watch?v=TvOOvH1l8Fc,https://github.com/ChenyuHeidiZhang/Spam-Slayer,,,"django, python, tensorflow",1,Johns Hopkins University,,,jzhan205,Johns Hopkins University,0,,,
Mathpix Challenge,https://hophacks-fall-2019.devpost.com/submissions/126743-mathpix-challenge,"Mathpix Latex Expansion

Docs

Created for Hophacks Fall 2019.

The purpose of this repo is to fulfill the Mathpix challenge to expand mathpix snip technology. Mathpix snip is a technology which processes handwritten or digitally printed advanced mathematical expressions and extracts Latex code from them. This process expands on mathpix snip technology to process larger data sets, like whole pages or several pages, rather than just a few lines which must be manually cropped (the current working stage of mathpix snip).

This project was created using publicly available APIs and Mathpix API found here: http://docs.mathpix.com

This application was built using Flask and Vue.js frameworks to showcase our Mathpix challenge. The Mathpix challenge was solved using a Google Cloud Vision Machine Learning API, OCR, which allowed us to detect text in any pdf and find the bounds of text for optimal cropping. So, larger data sets are deconstructed using OCR, processed by existing Mathpix technology, and then the extracted LaTeX code is reconstructed.
",,https://github.com/egzonarexhepi/mathpixlatexconverter,,"Winner of Mathpix Challenge (Mathpix), Best use of Google Cloud, Best Hack for Social Good (J. P. Morgan)","python, javascript, flask, vue.js, html5, css3, google-cloud, mathpix, ocr-web-service",1,"American University, Waterloo University, Rutgers University",,,egzonarexhepi,Boise State University,0,,,
celeAI.tech,https://hophacks-fall-2019.devpost.com/submissions/126775-celeai-tech,"Inspiration

Dyslexia is a language-based learning disability or disorder that includes poor word reading, word decoding, oral reading fluency and spelling. It is estimated that 1 in 10 people have dyslexia. We have witnessed many dyslexic people having to be dependent on others to a great extent when it comes to education. We aim to do something about this issue and make these people more independent. We came up with an idea to make Self-Learning possible for them.

What it does

We have created a website which takes in input of three formats, namely, Text, Speech and OCR images and gives output in the form of music and visual images. This is helpful to a great extent because people with dyslexia are a lot better with interpreting audio-visual data and music. It helps them create a link in their brain and remember that particular information for a longer period of time. 

How we built it

We first convert all out input format to text format. The Google-Cloud-Speech-to-Text API is being used to convert the speech into text. The Google-Cloud-Vision API is being used to convert the OCR images to text. Once we have all the inputs in text format, we are then converting that given text into music and a visual image. For the conversion of text to image, we came up with a python algorithm that can google search the given text and display the image search result and for the conversion of text to music we have taken the help of an API called DITTY that gracefully converts any given text to music. The supporting website's frontend is built with react and backend is built with flask. The backend is running on Google-Cloud and frontend is hosted on Domain.com.

Challenges we ran into

We came across many challenges while building this product. Firstly, we had planned to generate images from text using Generative Adversarial Networks. But due to the high time and space complexity, we could train only on a limited dataset which could not provide the desired results. Hence we had to come up with another algorithm to serve the purpose. Converting text to creative musical tunes was also very challenging.

Accomplishments that we're proud of

We are proud of the fact that we have managed to complete the proof of concept of the entire project and proper implementation of about 75% of the project in less than 36 hours. This project can prove to be of great help for the social good of people with dyslexia. 

What we learned

While doing this project, not only did we improve our technical skills but also learned a lot about dyslexia and how we can help them. We learnt how to work with various APIs, A great amount of troubleshooting and quite a bit about GANs and their implementation.  

What's next for celeAI.tech

We plan to work on our GAN model with better datasets and arrive at more accurate and creative results. Some research told us that dyslexic people will be able to read and interpret better if the words/letters are placed at certain angles. We would love to try and implement this on our website.
",https://www.facebook.com/manasa.rajesh.7/videos/1075051556026660/,http://celeai.tech,,"Best Domain Name from Domain.com, Best Healthcare Hack (Siemens), Best use of Google Cloud, Winner of FastForward U (FFU), Best Hack for Social Good (J. P. Morgan), Best Hack for a Philanthropic Cause (Bloomberg), Best Hack for Mental and Physical Well-being (Accenture)","google-cloud, google-vision, google-app-engine, domain.com, react, react-native, flask, python, redux, google-cloud-speech-to-text, ubuntu, github, git, material-ui",1,"Johns Hopkins University (1 member) , Vellore Institute of Technology(2 members)",Dell XPS,Domain.com|-|Google Cloud,MANASAR,Johns Hopkins University,2,radhikaagarwal2017,Naman-sopho,
MathPixHack: Braille Converter,https://hophacks-fall-2019.devpost.com/submissions/126838-mathpixhack-braille-converter,"HophacksFall2019

A simple python script to read a page of a pdf and use Mathpix API to convert the page into LaTex code which in turn is turned into Braille Text to aid visually impaired scientist/students. This project makes use of image manipulation through requests, numpy, and Pillow in python to create small images which the MathPix API can process, send them through the Mathpix API to be processed, and read the response to create a overall LaTex code. It then employs Braille font to create a Braille version of the pdf. The project saves 2 text files and 2 pdf files(given that the LaTex code had no error) which are the .tex/pdf copy of the compilied OCR from MathPix API and employment of Braille Script into the LaTex code.

Although several research is done in reading math equations as Braille Script, there is barely any working resource or libraries that help convert mathematical equations to braille. Even latex2mathml python package are full of bugs. As such due to the lack of framework to work with, a complete solution to braille conversion is out of the scope of the hackathon. A possible solution lies in conversion of LaTex to MathML and using the standard suggested by researchers to convert the MathML to Braille.

Further Readings on Braille and Mathematical Notation in Braille: https://www.researchgate.net/publication/221010006_Translating_MathML_into_Nemeth_Braille_Code https://scholarworks.rit.edu/cgi/viewcontent.cgi?httpsredir=1&article=1071&context=jsesd

To future readers, the project employs API from Mathpix that was available only during the hackathon so in order to use it, one would require the app id and app key for the project to work. Also, the project expects users to have TexWorks installed with the proper Path environment variable set in place to invoke pdflatex.exe and xelatex.exe to convert the LaTex code. This guide http://sachaepskamp.com/wp-content/uploads/2011/10/Install.pdf shows how to set the path variable. As for the braille fonts, they can be found in https://www.seewritehear.com/accessible-mathml/mathspeak/fonts/
",,https://github.com/RainbowMan1/HophacksFall2019,,"Winner of Mathpix Challenge (Mathpix), Best Hack for Social Good (J. P. Morgan)","python, tex",1,"Howard University; University of Texas at Dallas; University of Maryland, Baltimore County; Hunter College",,,RainbowMan1,"Howard University, The University of Texas at Dallas",1,RaghavBalasubramaniam,,
WordWordWord,https://hophacks-fall-2019.devpost.com/submissions/126839-wordwordword,"Inspiration

MS Marco challenge

What it does

Finish the user's search content in a search query. Suggest new search query.

How I built it

NLTK, keras, Word2vec, Machine learning.

Challenges I ran into

Understanding Embedding/Word vector.

Accomplishments that I'm proud of

Working LSTM model and program to predict word that users will use.

What I learned

NLP

What's next for WordWordWord
",,,,Winner of Microsoft Marco Challenge (Microsoft),"python, nltk, word2vec, keras",0,"University of Massachusetts Dartmouth, University of Massachusetts Dartmouth",,,HieuNgo,University of Massachusetts - Dartmouth,1,dkelch,,
Leap Alert,https://hophacks-fall-2019.devpost.com/submissions/126840-leap-alert,"Inspiration

Our team identified a specific problem in a large portion of the community - the lack of resources of available for allowing children with autism (or other similar communication/social impairments) to communicate safely and comfortably. One of our team members has a sibling with autism and can vouch for the difficulties those with autism face in communicating their needs effectively as well as the difficulties faced by the parents of those with autistic children. Busy parents fear leaving their child alone even for short periods of time.

What it does

Our software is a web application which connects to an external motion detection software (Leap Motion) in order to recognize simple hand swipe gestures to select certain commands. These commands serve to either communicate emergency concerns to the user's parents or to calm a user down if they are distressed. Those who will be using this software's gesture detection software will be those with autism. The inability for those with autism to communicate effectively prevents them from being able to efficiently alerting their parents of concerns through text or phone call. However, Leap Alert allows those with autism to form a basic connection between a hand gesture and a concern they may be able to communicate. Beyond this, the users with autism can also select from a list of videos which can calm them down. Those with autism often have certain, specific interests which can calm them if they are stressed and Leap Alert allows these users to access videos, audio clips, or images which might aid in this. The final product is a grid of options that the user can choose from. Rather than having to read text messages or call, the user can look at images and select images associated with concerns using hand gestures.

How we built it

Leap Alert is built on several API's and programming languages - namely, JavaScript, HTML, PHP, LeapJS, and Python. The application's software design is built around the Leap Motion software and sensor. The Leap Motion's motion detection software provided us the idea to recognize gestures to alert parents, so we needed to figure out how to receive and recognize gestures, and how to send out a message alerting parents. The former task involves understanding and implementing Leap Motion software and making use of LeapJS libraries. The latter task involved knowledge of how to send emails using PHP. This evolved into sending SMS with PHP with the aid of the Twilio public library. In order to have our application be able to be used directly with the motion detector in front of a computer where the user can see the results of their gestures in real time, JavaScript was needed. A combination of JavaScript and HTML was used to create a visual aid which depicts what options the user has to choose from. The JavaScript code had to work seamlessly with the data streaming from the motion detection software. Finally, to be able to send out an SMS with PHP code with JavaScript code, we required the use of the Python Flask library. This allowed us to create a server which runs Python using JavaScript. The Python code then runs a PHP file and sends out an SMS text alert.

Challenges we ran into

The process of creating a final product to demo was a difficult and tiring one. It required 6 laptops, all three Leap Motion sensors, and an Oculus Rift headset. In the beginning, we established that we would like the application we create to utilize the Leap Motion hardware and assist those with communication disabilities. However, it was quite difficult narrowing down the many ideas we had for how we could go about this. One idea we had was to use the Oculus Rift to train those with Autism how to utilize certain gestures to achieve certain results. Another idea we had was to make use of an Android phone application that those on the spectrum could use.
Aside from the process of idea creation, we ran into hardware problems as well. The Leap Motion software did not work very well on everyone's laptops. Issues with one laptop were solved by another, but it generated new issues. We checked out an AlienWare laptop to help process visual data better. Ultimately we narrowed down what we needed out of the hardware and chose which laptops would be most compatible for these needs. The challenges do not end there either! We were all new to PHP and getting an email and SMS to send was difficult. Then, getting JavaScript to run PHP code was also difficult! The internet claimed it was not easily possible to run PHP with JavaScript, but we used the Flask software to work around the incompatibility. There were also countless issues with debugging and making user interface smoother and finding the necessary software to download to our computers. However, we successfully made a demo to show what our application intends to do for those with autism. 

Accomplishments that we're proud of

The biggest accomplishment was most likely getting a working, clean demo with a team made mostly of first time hackers! We were also unfamiliar with PHP and Leap Motion technology, but we got accustomed to it just in time for creating our application. 

What we learned

We learned how to effectively make use of a diverse array of API's, how to use PHP, and how to conjure solutions to facilitating user experience. One of the most useful things we learned is how to use Python in a webpage (which conveniently allowed us to run PHP code with JavaScript!).

What's next for Leap Alert

Leap Alert was intended for children with autism. However, this technology can be extended to more than just those on the spectrum. Children with speaking disabilities, young children unable to speak, and even the elderly who have a hard time raising their voice can make use of an application such as Leap Alert to send out distress signals if they cannot easily speak up. As for the front end and back end aspects of our software, we would like to see Leap Alert be customizable for users (controlling how many grid squares the user has access to, what the external media is, etc). Furthermore, we would like to finish the design of the web application so that relevant images are actually displayed on the grid square (this is so that the users are not required to read what is written on the squares). 
",https://youtu.be/a_7Ots_CSyQ,https://github.com/siqicao/Leap-Alert.git,,"Best Healthcare Hack (Siemens), Winner of FastForward U (FFU), Best Hack for Social Good (J. P. Morgan), Best Hack for a Philanthropic Cause (Bloomberg), Best Hack for Mental and Physical Well-being (Accenture)","python, javascript, php, html, twilio, flask, leap-motion",1,Howard University; University of Maryland; Georgia Institute of Technology,Leap Motion,,scao,"University of Maryland - University College, Georgia Institute of Technology - Main Campus, Howard University",3,aupadhayay3,jdhendricks727,cryptonon
hophacks2019,https://hophacks-fall-2019.devpost.com/submissions/126841-hophacks2019,"hophacks2019

Using fitbit API to improve health related problems.

Description

This is a prototype built during hophacks 2019 of a software with the goal of using the fitbit API to monitor biometric readings given by the device to alert the users chosen individuals via sms messaging, using twillo, that the user is having an emergency. In it's current iteration its used to monitor for anxiety levels and panic attacks based off the users heart rate, sleep patterns, and calorie intake.

Collaborators

Elijsha

Marty

Rendell
",,https://github.com/speedykai/hophacks2019,,"Best Healthcare Hack (Siemens), Best Hack for Social Good (J. P. Morgan), Best Hack for Mental and Physical Well-being (Accenture)","jupyter-notebook, python, c++, makefile, batchfile, processing",1,"Germanna community college, George Mason University, Northern Virginia Community College",FitBit Ionic,,elijshamingus,"Germanna Community College, Northern Virginia Community College, George Mason University",2,speedykai,IntenseMarrow9,
SFNC CARPOOL,https://hophacks-fall-2019.devpost.com/submissions/126843-sfnc-carpool,"hophacks_carpool

This is a carpooling app called ""SFNC CARPOOL"" for the community in Baltimore, to mainly serve the transportation needs of students. It aims to provide updates and notifications for transportation for children to school and co-curricular activities and events. It also allows the community to keep track of the movement of children in general, and create networks of families and children with similar interests, while providing safe transportation for them.
",,https://github.com/lynetteljm/hophacks_carpool,,Best Hack for Social Good (J. P. Morgan),"java, xd",1,"Georgetown University, Singapore Management University",,,lynetteljm,Singapore Management University,1,trisfaulknerma,,
BaltiWatch,https://hophacks-fall-2019.devpost.com/submissions/126844-baltiwatch,"Inspiration

Past improper disposal, leaks, hazardous materials and wastes have resulted in tens of thousands of sites across our country that have contaminated our land, water (ground water and surface water), air (indoor and outdoor). This also happens in our neighborhood, which may even affect the health of our community. As part of the Baltimore community, we want to help with this issue by providing a report platform that makes the handling of trash, biohazards, etc. easier and quicker. Additionally, it helps to get our community involved in neighborhood cleanup by building a reward system. 

What it does

BaltiWatch is an Android app that helps the Baltimore community to group up and keep our neighborhood clean. Firstly, users can report trash, biohazards, etc. on our app. Then, BaltiWatch will map and list those hazardous waste cleanup locations, drill down to details about those cleanups and provide related information on the App so that users who are in the neighborhood can help with cleaning up and earn points by completing this activity. Besides, users can also earn points by verifying whether the reported issues have been solved. Finally, users can use these points to redeem different kinds of rewards that potential sponsors provide, such as Google Cloud Credit, Free ticket for FFC Monday dining, priority registration for courses, etc. 

How I built it

We built the app using android studio (Java for the backend, xml for the frontend) and used Google's Firebase Firestore to store our data online and Firebase Authentication for user authentication.  Additionally, we used git and github for collaboration.

Challenges I ran into

We had a lot of trouble getting started.  Setting up our project with Firebase authentication and Firestore for database took the entire first night.  Additionally, there were a few select features that took a disproportionately large amount of time.  For  example, it took us a long time to implement recycler views and Google Maps integration.

Accomplishments that I'm proud of

We felt an extreme sense of accomplishment watching eachother complete more and more features and our app become more and more complete.  Every new activity created and UI element successfully connected to the firebase backend was a step towards a more complete product that we could show off to judges.  What began as an empty screen with a button and 2 texfields that we were struggling to hook up to Firebase authentication eventually became the BaltiWatch that we will present today.

What I learned

We learned a lot about team dynamics and android development during Hop Hacks.  In the beginning of the hackathon, we had frequent merge conflicts.  However, we had almost none during the second half.  Additionally, it took us an entire night to set up firebase authentication and database, but only one day to complete everything else.  

What's next for BaltiWatch

The android app we have right now is a small part of a grander plan that we came up with at the beginning of this hackathon.  Adding photography to the platform would allow us to gather more useful data about the validity and seriousness of the reports.  Additionally, adding machine learning and computer vision to the platform would allow us to analyze the data and find trends and hotspots for littering, biohazards, etc.  Finally, a webapp would be useful to present these results to administrators in the government or nonprofits to better optimize their policies and actions for the community.
",,https://github.com/mason2smart/BaltiWatch,,"Best Healthcare Hack (Siemens), Best use of Google Cloud, Winner of FastForward U (FFU), Best Hack for Social Good (J. P. Morgan), Best Hack for a Philanthropic Cause (Bloomberg), Best Hack for Mental and Physical Well-being (Accenture)","java, firebase, android-studio, firestore",1,Johns Hopkins,,,mason2techie,Johns Hopkins University,3,ChristopherXu,jerrychen017,carolyli1014
E-Balance (The Environmental Bank),https://hophacks-fall-2019.devpost.com/submissions/126845-e-balance-the-environmental-bank,"Inspiration

With the world of technology expanding at an exponential rate, a certain toll is being taken on our earth. We wanted to unite technology and environment by bringing awareness to each individual's impact and contribution to climate change. Many people feel climate change can only be solved by legislation, however with small adjustments to our own daily lives, like eating less meat or taking public transportation, we can create large change.

What it does

The environmental bank (or E-bank) for short keeps track of your literal and monetary balance with earth. Every carbon footprint can be measured in a cost it would take to offset that footprint. There are many calculators that exist online that can profile how much your emissions are contributing to climate change. These calculators however are often overbearing and too detailed for people to take the time to fill out. We've simplified the interface by only showing a few common emission categories: travel (car, bus, plane), energy (washing machines, light), and food (meat and non-meat meals). 

We hope users enter their approximate usage in these different categories on a daily basis so they can see how their E-balance adds up. They also have the option of offsetting their emissions (either that week's or their total) by donating to a nonprofit that will support renewable energy sources. This, in the long term, will help renewable energy sources grow and become more affordable/accessible to people. 

The nice part about e-balance is that the total is never going to be overwhelming. We estimated (using free carbon calculators online) that a family of four would cost less than 3 dollars per week to offset (this was only including our three categories travel, energy, and food).

A feature we would like to add to our app is a ""Friends and Accomplishments"" page where users can track their statistics (like which category they use the most emissions in) and accumulate points to level up. One of the ways users will be able to earn points is by keeping a ""streak"" with the app (logging daily in any category). With the ability to connect with friends on the app, users will feel more enticed to level up and also introduce their friends to the app.

How we built it

We built this app on Xcode using Swift. This was our first time programming an app and programming in swift. It was a very new experience and it took a while to find our feet on the platform.

Challenges we ran into

Working in a new language is always a challenge. When we ran into bugs, it took much longer to debug. Additionally, there were many features we wish we could have added (like a scrollable ""picker"" to choose between Diesel or Petrol to make different category costs more personalized) but we did not have time to learn and debug those more complicated features.

Accomplishments that we're proud of

We're pretty proud to just have created an app since it was our first time. It was very cool working in a new environment with a new language. 

What we learned

How to use the basic features of Xcode and Swift. How to make a basic app.

What's next for Environmental Bank (E-Bank)

First the friends and accomplishments page where we can track their statistics. Second to add more categories and more subcategories (like what I mentioned above about fuel type for cars). Additionally, do more research on the actual calculation of the carbon balance. Right now the numbers we are using are quite crude (just the first values we found online), so we would like to do more research to make sure we are accurate with our calculations.
",https://youtu.be/c31pw2ogo5Q,,,Best Hack for a Philanthropic Cause (Bloomberg),"xcode, swift",0,Johns Hopkins University,,,klee223,Johns Hopkins University,1,petermgoh,,
Arborist Go!,https://hophacks-fall-2019.devpost.com/submissions/126846-arborist-go,"Arborist Go!- Scan the trees around you, report their health status and get rewards! 

Out of all the 3-1-1 requests reported within the 2010-2014 period made available on the Open Baltimore database, a tremendous 17,112 entries fell under the category of tree inspection, tree maintenance and fallen trees. To put this into perspective, within the past decade, there have been 407 tree-related deaths in the US alone. 

On top of all these cries for help, the city council staff have specifically pinpointed that tree surveys are a ‘very manual process’, that require time, financial and human resources. 

Another problem that came up repeatedly, 12,831 times in fact, was the problem of illegal dumping. In St .Francis’ raw surveys of the 21217 community, residents from as young as 6 to residents as old as 79 all faced the same issue of trash and garbage management, seriously affecting their quality of life-- and definitely not helping the rodent problem. 

Arborist Go! addresses the root causes of both these challenges. 

A gamified tree mapping and monitoring tool which leverages the ethos of traditional citizen science projects whilst maintaining a sustainable business model. As users register trees, update their monthly health status and earn points, they are incentivised and rewarded with the chance to win discounts and deal coupons for eco-friendly products from collaborating companies focused on environmental causes such as Beyond Meat, Lush Cosmetics and Stainless-steel Straws. 

Arborist Go! provides tree health status alerts in real time to the local council and environmental agencies to prioritise tree inspections and provides them with a database of updated vegetation information for urban planning, evaluation and research. 

How does it treat the trash problem though? It is the opinion of many that litter and dumping are not a logistical problem in Baltimore but rather, a deep running cultural problem. By encouraging community members to get out, start exploring and interacting with their neighbourhood in a positive way, they are becoming more aware of the environment around them, hopefully stimulating the cultural shift that is needed to tackle this trash problem.

Once you create a user account on the app, users are able to scan and register trees with their phone cameras and report the health status of that tree. The geolocation and health status of the tree is then stored in the database and the user is launched into a game experience with the chance to win discount and deal coupons which are stored in their in-app wallet, redeemable at participating stores. To ensure that it is in fact a tree that is being scanned and that a single tree is only registered on the database once, the google vision API for tree recognition is employed as well as only allowing the registration of a single geolocation. 

Technologies used 
NodeJS web app, using jquery

Google vision API for tree recognition

Google map API for geolocation of user+tree

Firebase with mail authentication

Bootstrap for rendering dynamic and mobile friendly pages

Future steps

Complete database

API for detecting direction and distance of a tree

Exclusive tree detection

Better user experience
",,https://github.com/xj-m/Arborist-Go,,"Best Automation Hack with UiPath, Best use of Blockstack, Best Domain Name from Domain.com, Winner of Mathpix Challenge (Mathpix), Best Healthcare Hack (Siemens), Best use of Google Cloud, Winner of FastForward U (FFU), Best Hack for Social Good (J. P. Morgan), Best Hack for a Philanthropic Cause (Bloomberg), Best Hack for Mental and Physical Well-being (Accenture)","html, javascript, css",1,"JHU, NYU",,,xj-m,"Johns Hopkins University, New York University",3,anhaotian1996,sdh381,zlin32
VVL (Visual Verbal Learning),https://hophacks-fall-2019.devpost.com/submissions/126848-vvl-visual-verbal-learning,"Inspiration

We wanted to build a learning tool that made a huge social change and upon researching we found a condition called ""Aphantasia."" This condition entails people who deal with blindness in their mind. We were able to then apply our idea originally meant for helping language learners and visual learners for people who also deal with aphantasia. 

What it does

Our project is a website that allows a user to enter a string of text through typing or speaking through a microphone. The site then pulls key words from each sentence to display to the user images that represent the key words in their sentence. This outputs on the screen helping with learning new words with images. 

How we built it

We used Angular for the front end and Python for the back end. We also used Natural Language Processing to analyze the text and categorize the text. In addition, we used Google Custom Search Engine to make queries that return images based on the categories of the NLP. 

Challenges we ran into

We had challenges implemented Google Speech to Text API. 

Accomplishments that we're proud of

We are proud to be able to tackle something not widely focused on in the tech field and medical field. 

What we learned

We learned how to improve our empathy and best help others.

What's next for VVL (Visual Verbal Learning)

We want to implement Speech to Text and Text to Speech for the application. We also plan to develop a mobile application.
",,https://github.com/saheedandrew/TextVisualizer,,"Best use of Google Cloud, Winner of FastForward U (FFU), Best Hack for Social Good (J. P. Morgan)","angular.js, python, flask, google, google-custom-search, google-image-search, google-web-speech-api, natural-language-processing",1,"University of Oregon, Brooklyn College (2), Johns Hopkins University",,,andrewsaheed,"Brooklyn College of the City University of New York, University of Oregon, Johns Hopkins University",3,jpeter17,ayodelefarinre,nsarmoe1
BMoreSafe,https://hophacks-fall-2019.devpost.com/submissions/126849-bmoresafe,"Inspiration

Was sadden by the poor quality of life while travelling to the campus on the bus. We wanted to aid the community in any way we can.

What it does

Our website has a game that is targeted on to the youth that teaches them the dangers of substance abuse. We also have information/resources for people who are currently struggling with a substance abuse. 

How we built it

We used RenJS to build the game. The website built with HTML and CSS

Challenges we ran into

Attempting to run a local server but scrapped that idea as it wasn't necessary

Accomplishments that we're proud of

Polished website, everything runs well

What we learned

RenJS, Maps

What's next for BMoreSafe

Improvement on the game such as sounds, facial expression, and sound effects. 

Best Domain Name Submission

http://bmoresafe.online/
",,https://github.com/girikification/bmoresafe,,"Best Domain Name from Domain.com, Best Hack for Social Good (J. P. Morgan), Best Hack for Mental and Physical Well-being (Accenture)","html5, css3, renjs, baltimore-open-data",1,"Temple University, Brooklyn College, University of Maryland, University of Waterloo",,,pokelegocuber,"Brooklyn College of the City University of New York, University of Maryland - College Park, Temple University, University of Waterloo",3,theandrewhong,chiuannica,girikification
Mathpix PDF/Image Parser,https://hophacks-fall-2019.devpost.com/submissions/126850-mathpix-pdf-image-parser,"Inspiration

Having been interested after hearing about Mathpix in the past, we decided that it would be a good idea to take this opportunity to both potentially improve this product and further our own understanding of coding.

What it does

Our program takes the inputted document (normally a PDF but a small modification allows it to be either JPG or PNG) and breaks it along horizontal blank lines. The blank lines are found by detecting when the average grayscale value of a row is greater than or equal to the median of the average grayscale value of all the rows, which is a rough but workable solution. 

Following this, the image is then split into different horizontal blocks; each run of non-blank lines, along with the two runs of blank lines adjacent to it, form a horizontal slice of the original image. The relevant cropping data is then sent to the Mathpix OCR API, along with the encoded image data, which returns Mathpix's interpretation of each individual block of text from the image. This is then printed out along with a new line character at the end, to ease copying of the LaTeX representation into another document. This works surprisingly well in converting PDFs, such as math papers, into both text and LaTeX. 

How I built it

This program was coded in Python, using the Mathpix OCR API as well as several modules. Namely, pdf2image was used to convert the inputted PDF into a JPG format, which was necessary, as both the Mathpix API and the parsing program required either JPG or PNG. Helping each other bug-test and put together the overall program, we successfully implemented our original idea, with a few improvements, and were able to parse a full-sized PDF document into LaTeX.

Challenges I ran into

We ran into several challenges with regards to programming, both in the case of creating an algorithm that worked and interacting with the API to obtain the results we needed. This included making sure the blank-line-recognition algorithm worked successfully in most cases and interacting with the API in a way that allowed us to obtain the information we needed.

Accomplishments that I'm proud of

We're proud of working together as a team and managing to implement our original idea for this project. Furthermore, we're extremely happy with how well our image slicing algorithm worked, especially since the main idea was only slightly changed throughout the course of the project.

What I learned

We learned how to collaboratively work together on a coding project as well as using several libraries and APIs together with each other.

What's next for Mathpix PDF/Image Parser

Our program could be improved in several ways, namely the detection of lines of text and the cropping of the image into readable blocks, but the fundamental idea of breaking up a document horizontally to allow the Mathpix API to interpret each line of text or math equations should be sound.
",,https://github.com/Nimbers/Mathpix-Hophacks-2019,,Winner of Mathpix Challenge (Mathpix),"python, mathpixocr",1,Johns Hopkins University and University of Maryland,,,soconn19,"Johns Hopkins University, University of Maryland - College Park",3,jefferyzhouehs,F28L,ngupta41
StressedDesert,https://hophacks-fall-2019.devpost.com/submissions/126851-stresseddesert,"Inspiration

During the HopHacks Hackathon, our team was considering the millions of families in the US who live in areas known as “food deserts”. 

What it does

Food deserts are regions in which affordable access to groceries is limited. In order to raise awareness, we scraped hundreds of webpages to produce a dataset of 24,300 grocery stores across the country and a dataset of 12,000 food banks across America. Through our web app, our users will be able to see food desert areas and find food banks that support those who need it most.

How we built it

We used Python's Beautiful Soup library to scrap the hundreds of chains and locations listed on www.supermarketpage.com as well as www.feedingamerica.org/find-your-local-foodbank and foodpantries.org.
After cleaning and sorting the raw HTML, we used Google's geocoding API to look up the latitude and longitude of each address of our grocery store and food bank data points.

In order to display our data set through a web application, we embedded a heatmap of the store data and markers of the foodbanks with the Google Maps API. 

Challenges we ran into

Web scraping is a picky, messy business. So is learning new documentation on the fly. Through teamwork and dedication, we were able to clean the data to an acceptable standard, although long hours were spent fiddling with types and IndexErrors.

Also, our incredibly large dataset took an incredibly long time to process.

Accomplishments that we're proud of

We are proud of how well our team worked together.

What we learned

We learned how to use Python Beautiful Soup, the frustration of web crawling (that website were not built to be crawled but we did it anyway). Lastly, API calls are always expensive. 

What's next for StressedDesert

We would like to spend more time improving data relevance and quality. Although we found ample information to be gathered, our next steps include updating and validating the data.
",,https://github.com/arschilke/StressedDeserts,,"Best Healthcare Hack (Siemens), Best use of Google Cloud, Best Hack for Social Good (J. P. Morgan), Best Hack for a Philanthropic Cause (Bloomberg)","python, javascript, html, css, node.js, express.js, google-maps, google-geocoding, beautiful-soup, domain.com",1,Loyola University Maryland,,Google Cloud,arschilke,Loyola University Maryland,2,APierce-Ptak,jckitson,
Meal.ly,https://hophacks-fall-2019.devpost.com/submissions/126852-meal-ly,"Inspiration

At the St Francis Community Center, people have seen so many cases of how children are saved from death, illnesses, and bad school performance from being able to feed themselves well. The children not enrolled in such programs are much less luckier. However, just a few miles away, the food wastage in supermarkets, groceries, and even that compiled in NGOs, are as prevalent as other cities in the US.

What it does

The app matches excess food with people that need it. The restaurants and organizations agreeing to give out free food at the end of each day have their food kinds and amounts scanned and input by the camera and the image recognition of our app, and all users are alerted of food sources within 15 mins of their closing time.

How we built it

We built the frontend using Android Studio and its native design element and the backend using flask. The image recognition function is realized using Google Cloud Vision API and the communication between frontend and backend is via and open sourced library called okhttp3.

Challenges we ran into

All of our team members completely don't have any app development experiences before

Accomplishments that we're proud of

The things we learned

What we learned

Everything being listed above

What's next for Meal.lly

Verification login page and Operation aspects between offline and online
",,https://github.com/jzheng17/Hophacks,,"Best use of Google Cloud, Best Hack for Social Good (J. P. Morgan), Best Hack for a Philanthropic Cause (Bloomberg)","java, python, flask, android, google-cloud-vision, okhttp3",0,johns hopkins,,,jzheng17,Johns Hopkins University,1,arielbr,,
Macro,https://hophacks-fall-2019.devpost.com/submissions/126853-macro,"Inspired by our journey of bodybuilding, diet has been seen as an integral aspect of losing weight and building muscle. As nutritional information is rarely taught in grade school, many are unaware of the amount of calories or macronutrients in everyday food. Macro was created as an easy tool for others to find out the nutritional information for food with just a click of a finger. The application utilizes Google Cloud Vision API and queries through the USDA food composition database to obtain nutritional info and display on the application. The application also utilizes Firebase to allow users to store their data and recent activity. The front end of the application was created through React Native allows for dual compatibility with Android and IOS devices.
",,,,"Best Healthcare Hack (Siemens), Best use of Google Cloud, Best Hack for Social Good (J. P. Morgan), Best Hack for Mental and Physical Well-being (Accenture)","react-native, firebase, javascript, google-cloud-vision-api",0,"University of Maryland, College Park",,Google Cloud,owenluo,University of Maryland - College Park,2,patrickcao,rzou876,
Operator In-Loop Novelty Labelling,https://hophacks-fall-2019.devpost.com/submissions/126854-operator-in-loop-novelty-labelling,"Inspiration

Machine learning is a growing presence in intelligence, surveillance, and reconnaissance (ISR) particularly in applying neural network object detectors, localizers, and trackers to find threat objects such as visible/covert explosive devices or firearms. Even though I acknowledge the power of augmenting human judgment with AI, a major problem is that machine learning models struggle to adapt as the problem space changes. For example, a shift in the appearance and type of materials used to make improvised explosive devices could be catastrophic to a static model. Often you will want the organization deploying the AI to send the data back to the research team to update to ML. However, if the model has been running for a month, the research team has to watch a whole month of video footage.

The big question: what can you do to better your chances at getting the best improvement to your AI as fast as possible?

The big solution: have operators collect only the most valuable data while they are fielding it 

What it does

It is a mimic display that deploys MobileNet, a preconfigured neural network object detector. My display, however, augments the typical view with addition features that allow the operator to choose what data to save to disk.


a button to start recording video to disk and declare a new threat object
an input box for annotating a novel threat object
a bar graph for prioritizing the most frequently seen (and therefore more pressing) novel threat objects


How we built it

We set up a PiCam/RaspberryPi pair to work as a telemetry station and stream video over a socket.
Our laptop receives the data and connects it to a dashboard built in Plotly/Dash.
On the backend, the dashboard runs object detection on the video feed and updates the display if an object is found (Keras/Tensorflow).

Challenges we ran into

Sockets. Getting 2 separate devices to communicate with each other fluidly and reliably.
Getting everything to run real-time. How much do we have to slow the ML down to maintain a real-time video? Is this rate of ML good enough?

Accomplishments that we're proud of

IT ALL WORKS

What we learned

We will update this later.

What's next for Operator In-Loop Novely Labelling

We will update this later.
",,https://gitlab.com/chiwei.zhang/2019_hophacks_isr,,,"python, keras, raspberry-pi, socket, tensorflow, picam, plotly",0,Johns Hopkins University,,,wzhang53,Johns Hopkins University,1,dsamson4,,
Mental Health Checker,https://hophacks-fall-2019.devpost.com/submissions/126855-mental-health-checker,"Inspiration

What it does

How I built it

Challenges I ran into

Accomplishments that I'm proud of

What I learned

What's next for Mental Health Checker

Inspiration: a friend of mine was diagnosed with paranoi last semester. However, she was

What it does: this mental health checker provides a relatively reliable diagonosis with less than 30 questions.
",,https://github.com/pli28/Hophacks19,,,"html/css/javascript, python",1,Johns Hopkins University,,,pli28,Johns Hopkins University,0,,,
"Prediction of ADRs using 3D Data, GE data, and Extra Trees",https://hophacks-fall-2019.devpost.com/submissions/126856-prediction-of-adrs-using-3d-data-ge-data-and-extra-trees,"Inspiration

After viewing recent advances in predicting adverse drug reactions along with those in 3D small molecule alignment, we were inspired to attempt to couple the two. 

What it does

We leverage 3D structure and gene expression data to show improvements in accuracy and significant improvements in training speed for adverse drug reaction prediction using extremely randomized trees. 

How we built it

We built it in python, using scikit-learn, several packages including multiprocessing and LS-Align.

Challenges we ran into

We had difficulty with the general time requirements for certain steps. We couldn't use flexible alignment as our devices lacked sufficient processing power.

Accomplishments that we're proud of

The general speed up we achieved as well as our integration of multiprocessing to speed up the overall program.

What we learned

We learned that even in a significantly reduced form, 3 dimensional data has the potential to significantly impact ADR predictions. 

What's next for Prediction of ADRs using 3D Data, GE data, and Extra Trees

We hope that more research can be done on the topic of including 3D data. 
",,https://github.com/mpavlak25/HopHacksFall2019,,"Best Healthcare Hack (Siemens), Best Hack for Social Good (J. P. Morgan), Best Hack for Mental and Physical Well-being (Accenture)","python, ls-align, scikit-learn, pandas, numpy, multiprocessing",0,"JHU, NIH",,,mpavlak25,Johns Hopkins University,1,brandonmliu,,
G-Rated,https://hophacks-fall-2019.devpost.com/submissions/126858-g-rated,"Inspiration

When I (Julian) was in my OOSE class (Object Oriented Software Engineering) class at the start of the semester. The first thing my professor had us all do was come up silly ideas that could eventually become semester long projects. The only one I thought of then was a word generator to help me come up with other silly ideas. However, as I studiously labored at coming up with silly ideas, I came up with one that, though I didn't end up pursuing it in class, I could not let go of. The idea was to have a ML model automatically detect beer cans in a picture, and remove them from the image somehow. Now I don't need this (nor did I ever to be honest), but it just sounded like _ fun _.

What it does

We trained a model that uses Tensor flows to recognize beer cans in images, and automatically processes the image and blur out those nasty cans. 

How we built it

We used Tensor Flows and an imageAI library we found to help us along the way as well as a GPU we had in the basement to use as a server.

Challenges we ran into

First of all, we needed a dataset of over 200 annotated photos with beer cans in them. Next, we needed to program the neural net so that we could get started. Finally, we spent a lot of time trying to use different cloud computing set ups, but eventually resorted to our own hackneyed ways of doing things. 

Accomplishments that we're proud of

Honestly, we are so proud that our model exists. It didn't seem likely that our ideas would come to fruition today. Furthermore, we're really proud of our dataset. We set up a script to scrub Google Images for beer cans, then manually sifted out the chaff from 650 odd photos, then annotated each image one by one to ensure quality as well as quantity.

What we learned

We learned a whole heck of a lot! First of all, this software is super powerful and we definitely want to try our hands at it again soon. Next, we realized it would take a lot more time than anticipated to properly set up cloud computing for our purposes. 

We learned the true value of pre-made datasets.

What's next for G-Rated

We viewed this project as sort of a Proof of Concept for other things we can do with this tech. For example, we've already been talking about applying it to a whole bunch of different situations. However, we plan on continuing down this path, and eventually be able to replace beer cans with soda cans using just our AI.
",,https://github.com/Drmakneeo/grated,,Best use of Google Cloud,"tensor, python, nvidia, imageai, flask, neuralnet",1,Johns Hopkins University,,,jorillac105,,1,Mateoabascal,,
Bridge,https://hophacks-fall-2019.devpost.com/submissions/126859-bridge,"Inspiration : The striking gap between Non-profits, community and potential corporate sponsors.

What it does : Application that allows Non-profits in various sectors, volunteer/user community and corporate sponsors actively engage with each other. Non-profits push event notifications to community. Volunteers can raise relevant issues as suggestions to various Non-profits belonging to a specific cause.Recommendation engine to show ustom recommendations of Non-profits and events based on interests in social causes and proximity are created. Corporate sponsors can easily identify credible NPOs to support and collaborate with.

How we built it :Backend : Neo4j Graph database analytics and visualization; Cypher query language ; Python

Frontend : Android studio  + Realtime database hosted on Firbase ; Google maps API on the mobile App

Challenges we ran into : Limited prior experience in front end development; Lack of Relevant datasets to work with;

Accomplishments that we're proud of : Great technical learning in a very short time.

What we learned : Graphical analytics, android app development, google maps api, teamwork and time management

What's next for Bridge : Would like to create more features that are interactive and can scale up to various countries in the world and address grass-root level community challenges.
",,https://github.com/varanasivenkatsai/Bridge,,"Winner of FastForward U (FFU), Best Hack for Social Good (J. P. Morgan), Best Hack for a Philanthropic Cause (Bloomberg)","neo4j, android-studio, python, firebase, google-maps, sql",1,University of Florida,,,SrivalliBoddupalli,University of Florida,2,knagaraj,varanasi,
Propel,https://hophacks-fall-2019.devpost.com/submissions/126861-propel,"Inspiration

I came across some interview responses from local Baltimore residents and found that most of them are concerned about the issue of violence around them, so I wanted to hack something that may help mitigate that problem. 

What it does

The app lets people post activities that they are interested in doing but might not have the means to. I believe that each individual has something interesting to teach so I wanted to provide a platform for people to give and gain knowledge and skills from one another. Exercise and hobby-related classes tend to be very pricy. I believe that if my idea were to be put into practice, it would open a lot of doors for many groups of people to learn things they are interested in. Not only that, people could build tighter-knit communities as they spend more quality time with one another. As the activities are meant to be for self-enrichment purposes, young people who participate could benefit in the long-term and possibly add to their career. It would also help them stay out of trouble to some extent.

How I built it

React + Firebase

Challenges I ran into

I wasn't able to complete all the features I wanted to build

Accomplishments that I'm proud of

I coded non-stop since  I got to start at 10pm last night.

What I learned

It's so hard to settle on one idea

What's next for Propel

A lot of ideas in my head but if I discuss them now I might miss the submission deadline 
",,,,"Best Hack for Social Good (J. P. Morgan), Best Hack for a Philanthropic Cause (Bloomberg), Best Hack for Mental and Physical Well-being (Accenture)","react, firebase, javascript, html5, css3",0,university of virginia,,,lanasta,University of Virginia,0,,,
Don't Jump,https://hophacks-fall-2019.devpost.com/submissions/126862-don-t-jump,"Inspiration

The past week was national suicide prevention week. All of us go through some tough times when we feel like the world is bearing down upon us. People often don't feel comfortable in sharing issues in person. Using such a service in the initial stages easier than calling a helpline, more accessible. We wanted to do something to motivate people to seek help instead of feeling despair. 

What it does

It is an SMS based service that automatically detects whether the user is seriously contemplating suicide from conversations. We receive text messages, parse it and assign it a score. Since there are various levels to such an intent, we calculate a score and suggest actions according to it.

How I built it

We deployed a Twilio app to handle receipt and delivery of messages. Our custom Flask server parses texts received on this app's phone number. Each user's conversations are handled differently and responses are customised as per a score we calculate. This score represents the likelihood that a user is serious about committing suicide or inflicting self-harm. We use Microsoft's GEN Encoder API to generate representations for each text. To stress on newer texts, we weight the score of each message received--a newer text gets higher weightage. 

Taking into account conversational context and history is important since it is unlikely that a user will decide to inflict self-harm and send out a text. It is likely that such an emotion is built up over time.

Challenges I ran into

Effective conversational APIs are unavailable publicly. It was difficult to model multi-turn conversations. Finding a good scoring model was also crucial and time-consuming.

Accomplishments that I'm proud of

I'm happy we found a way to consider multi-turn dialogues since most systems like this one just take into account one one text. Coming up with an effective model to assign scores for user intents was also impactful. Working on a service that might help improve mental health personally excites us.

What I learned

I learned to use various APIs, fast deployment and putting into use NLP tools on offer.

What's next for Don't Jump

This can be offered as an API service by existing providers that work in a similar space (eg: Woebot). It can be part of a wider suite of tools.
",,,,"Winner of Microsoft Marco Challenge (Microsoft), Best Hack for Social Good (J. P. Morgan)","twilio, python, natural-language-processing, azure, heroku",0,Johns Hopkins University,,,ykl7,Johns Hopkins University,1,anuraagbaishya,,
SafeRoutes,https://hophacks-fall-2019.devpost.com/submissions/126863-saferoutes,"Inspiration

We have noticed that there is a large presence of crime in Baltimore, MD and were wondering how people got home safely. We wanted to leverage the power of big data to help others get to their desired destination safely.

What it does

The user defines their desired destination and the app will then reveal alternative directions. All directions will be labeled either green or red to indicate whether that path is safe or not.

How we built it

We implemented a modified version of Dijkstra's Algorithm, by giving weights to the different paths we were able to generate. The weights were determined by the amount crime that has occurred throughout that path. The path with the lowest weights will be labeled green while the paths with the higher weights will be labeled red.

Challenges we ran into

One of our team members left the group without any notification. Although we were understaffed, we were still able to produce a working product.

Accomplishments that we're proud of

Determining optimal routes for the desired destination and applying weights to different paths.

What we learned

We were able to solidify our understanding of Dijkstra's Algorithm and its implementation

What's next for SafeRoutes

We plan to further optimize our algorithm to determine the shortest yet safest paths and make the UI more user-friendly and lighter for faster performance. We plan to ship the app to the app store.
",,,,"Best Hack for Social Good (J. P. Morgan), Best Hack for a Philanthropic Cause (Bloomberg), Best Hack for Mental and Physical Well-being (Accenture)","swift, java",0,Georgetown University Lehigh University,,,yc767,"Georgetown University, Lehigh University",1,andrewha428,,
